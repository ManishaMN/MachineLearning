---
title: "XGBoost"
author: "Manisha"
date: "2023-11-25"
output: html_document
---

##  load libraries
```{r cars}

library(tidyverse)
```

## loading ames data , Splitting data
```{r split}
library(AmesHousing)
ames <- make_ordinal_ames()
ames <- ames %>% mutate(id = row_number())
set.seed(4321)
training <- ames %>% sample_frac(0.7)
testing <- anti_join(ames, training, by = 'id')
```

#after initial exploration, selected variables
```{r}
training <- training %>% 
  select(Sale_Price,
         Bedroom_AbvGr,
         Year_Built,
         Mo_Sold,
         Lot_Area,
         Street,
         Central_Air,
         First_Flr_SF,
         Second_Flr_SF,
         Full_Bath,
         Half_Bath,
         Fireplaces,
         Garage_Area,
         Gr_Liv_Area, 
         TotRms_AbvGrd)
```

#To build this model in R we first need to get the data in the right format. We need separate data matrices for our predictors and our target variable. First, we use the model.matrix function will create any categorical dummy variables needed. We also isolate the target variable into its own vector.

```{r}
train_x <- model.matrix(Sale_Price ~ ., data = training)[, -1]
train_y <- training$Sale_Price
```

#we will use the xgboost package and the xgboost function. 
#First we will set the seed to make the results repeatable. 
#Then, with the xgboost function we give the predictor variable model.matrix in the data = option. 
#The label = option is where we specify the target variable. 
#The subsample = 0.5 option is specifying that we will use stochastic gradient descent and only use a random 50% of the training data for each tree. The number of trees (nrounds) is set to 50.

```{r}
library(xgboost)

set.seed(12345)
xgb.ames <- xgboost(data = train_x, label = train_y, subsample = 0.5, nrounds = 50)
```
#That will build a model for us, but we need to start tuning the model. For tuning the model, we will use the function xgb.cv. This function will tune the number of trees (nrounds) variable. The rest of the inputs are the same as the xgboost function above. The only additional option is the nfold = option that sets the number of folds in the cross-validation.

```{r}
xgbcv.ames <- xgb.cv(data = train_x, label = train_y, subsample = 0.5, nrounds = 50, nfold = 10)
```
##left hand column is the RMSE from all the models built on the training data. As the number of trees increases, the RMSE is getting better and better which isn’t surprising as the errors from one will inform the next tree. However, the right hand column is the RMSE on the validation (test by name above) data, which is not guaranteed to improve as the number of trees increases. This validation evaluation will help us evaluate the “best” number of trees. We can see that the validation RMSE is minimized at 24 trees.



#Now that we know that 24 trees in the model is the optimized number (under the default tuning of the other parameters) we can move to tuning the other parameters. We will again use the train function from caret. We will use many different parameters in the tuning. 
#Inside the tuneGrid option we will fix the nrounds to 24 (as determined above), the gamma value to 0, and the colsample_bytree and min_child_weight to be 1. 
#We will change the eta value to be one of 5 values, max_depth to be one of 10 values, and subsample to be one of 4 values. 
#Inside the train function we will still use the 10 fold cross-validation as before. For the method option we have the xgbTree value. The plot function will provide a plot comparing all of these tuning parameter values.

```{r}
library(caret) 

tune_grid <- expand.grid(
  nrounds = 24,
  eta = c(0.1, 0.15, 0.2, 0.25, 0.3),
  max_depth = c(1:10),
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(0.25, 0.5, 0.75, 1)
)

set.seed(12345)
xgb.ames.caret <- train(x = train_x, y = train_y,
      method = "xgbTree",
      tuneGrid = tune_grid,
      trControl = trainControl(method = 'cv', # Using 10-fold cross-validation
                               number = 10))

plot(xgb.ames.caret)
```


#The lowest RMSE on cross-validation occurs where a maximum tree depth is 6, the subsample is 100% (not stochastic; has the whole sample), and eta is 0.25.

```{r}
xgb.ames.caret$bestTune
```
#To truly optimize the XGBoost model, we would take these values and try to optimize the nrounds as we did before under these new values for the other parameters. We would then redo the tuning of these other parameters under the new number of trees and shrink the grid to get more exact values of each tuning parameter. This process is rather time consuming so be prepared.


#variable importance
Gain - equivalent metric to random forests
Coverage - measures the relative number of observations influenced by the variable
Frequency - percentage of splits in the whole ensemble that use this variable
```{r}
xgb.ames <- xgboost(data = train_x, label = train_y, subsample = 1, nrounds = 24, eta = 0.25, max_depth = 5)
xgb.importance(feature_names = colnames(train_x), model = xgb.ames)
```
##Variable importance plot
#From the output above we see the variables in the model ranked by gain for variable importance. 
#The plot above is also in terms of gain. The nice part about the plot is that it clusters the variables together that have similar gain. From the above output we have 3 clusters of variables. The first cluster has Year_Built all by itself. The second cluster has Gr_Liv_Area, Garage_Area, and First_Flr_SF. The last cluster has all the remaining variables.
```{r}
library(Ckmeans.1d.dp)
xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.ames))
```

## Variable selection using random variable
#random variable in R we will just use the rnorm function with a mean of 0, standard deviation of 1 (both defaults) and a count of 2051 - the number of observations in our training data.

```{r}
training$random <- rnorm(2051)

train_x <- model.matrix(Sale_Price ~ ., data = training)[, -1]
train_y <- training$Sale_Price

set.seed(12345)
xgb.ames <- xgboost(data = train_x, label = train_y, subsample = 1, nrounds = 24, eta = 0.25, max_depth = 5, objective = "reg:linear")

```
```{r}
xgb.importance(feature_names = colnames(train_x), model = xgb.ames)
xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.ames))
#7 variables that are below the random variable in terms of gain. These could be considered for removal from the model. However, We can see that there are 3 variables above random, but considered to be similar to random based on the cluster they are in. You could consider to remove these as well. As always though, the main focus is on prediction here and not necessarily on model parsimony.
```
##Interpretability using partial dependence plots
#This nonlinear and complex relationship between Garage_Area and Sale_Price is similar to the plot we saw earlier with the GAMs and random forests. This shouldn’t be too surprising. All of these algorithms are trying to relate these two variables together, just in different ways.
```{r}
library(pdp)

xgb.ames <- xgboost(data = train_x, label = train_y, subsample = 1, nrounds = 24, eta = 0.25, max_depth = 5, objective = "reg:linear")
pdp::partial(xgb.ames, pred.var = "Garage_Area", 
        plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = "lattice", 
        train = train_x)
```

